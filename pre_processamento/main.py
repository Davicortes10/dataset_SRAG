from pre_processamento import PreprocessDataset
from data_lake import Data_Lake
from dados_faltantes import Dados_Faltantes
from gcp_dataset import GCP_Dataset
from pre_processamento import PreprocessDataset
from outliers import Outliers
from normalizacao import Normalizacao
from sqlalchemy import create_engine

class Oficial:
    """
    Classe principal para gerenciar o processo de ETL (ExtraÃ§Ã£o, Tratamento e Carregamento de dados).
    
    Etapas:
    1ï¸âƒ£ LÃª os dados do GCP atravÃ©s da classe `GCP_Dataset`.
    2ï¸âƒ£ Trata os dados faltantes usando a classe `Dados_Faltantes`.
    """

    def __init__(self):
        self.df = None
        self.bot_passo_1 = Data_Lake("/home/davicortes_oliveira1/dataset_SRAG/pre_processamento/srag.csv","srag_datalake")  # Inicializa sem valor 
    
    def data_lake(self):
        self.bot_passo_1.executar_datalake()

    def ler_dataset(self):
        """
        LÃª os dados do banco GCP e armazena no atributo `self.df`.
        ApÃ³s a leitura, inicializa `bot_passo_2` com `Dados_Faltantes(self.df)`.
        """
        print("ğŸ“¥ Lendo o dataset do GCP...")
        gcp = GCP_Dataset()
        self.df = gcp.ler_gcp_DB()

        if self.df is None or self.df.empty:
            print("âŒ Erro: O dataset nÃ£o foi carregado corretamente!")
        else:
            print("âœ… Dataset carregado com sucesso!")
            bot = Normalizacao(self.df)
            dados_faltantes = Dados_Faltantes(self.df)  # Agora pode ser inicializado corretamente
            self.df = self.tratar_dados_faltantes(dados_faltantes)
            self.df = bot.normalizar_sexo_sinto(self.df)
            self.df = self.pre_processamento()

    def tratar_dados_faltantes(self, dados):
        """
        Executa o tratamento de dados faltantes.
        Verifica se o dataset foi carregado antes de iniciar o processamento.
        """
        if self.df is None or self.df.empty:
            print("âš ï¸ O dataset ainda nÃ£o foi carregado. Execute `ler_dataset()` primeiro.")
            return
        
        print("ğŸ”„ Iniciando o tratamento de dados faltantes...")
        self.df = dados.executar_tratamento_de_dados_faltantes()
        print("âœ… Tratamento de dados concluÃ­do!")

        return self.df
    
    def pre_processamento(self):
        pre = PreprocessDataset(self.df)
        self.df = pre.converter_tipos_colunas()
        return self.df
    
    def outliers(self):
        oute = Outliers(self.df)
        self.df = oute.executar_outliers()
        self.df = self.pre_processamento()
    
    def normalizacao(self):
        bot = Normalizacao(self.df)
        self.df = bot.executar_normalizacao()
        self.df = self.pre_processamento()

    def enviar_para_gcp(self, df, if_exists="replace"):
        """
        Envia um DataFrame Pandas para uma tabela MySQL no Google Cloud (GCP) sem dividir em chunks.

        ParÃ¢metros:
        - df (pd.DataFrame): DataFrame a ser enviado para o banco de dados.
        - if_exists (str): OpÃ§Ã£o de escrita na tabela ('fail', 'replace', 'append'). PadrÃ£o: 'replace'.

        Retorna:
        - None
        """

        print("\nğŸ” Iniciando envio do DataFrame para o GCP...\n")

        # ğŸ“Œ Verificar se o DataFrame estÃ¡ vazio antes de enviar
        if df.empty:
            print("âš ï¸ Aviso: O DataFrame estÃ¡ vazio. Nenhum dado serÃ¡ enviado para o banco de dados.")
            return

        if len(df.columns) == 0:
            print("âš ï¸ Aviso: O DataFrame nÃ£o possui colunas. Verifique os dados antes do envio.")
            return

        # ğŸ“Œ Exibir os 5 primeiros registros para depuraÃ§Ã£o
        print("\nğŸ“Š Primeiros registros do DataFrame que serÃ¡ enviado:")
        print(df.head())

        try:
            # Criar a conexÃ£o com o banco de dados
            print("\nğŸ”— Conectando ao banco de dados GCP MySQL...")
            engine = create_engine("mysql+pymysql://devdavi:12345678@34.170.252.6/srag_warehouse")

            # ğŸ“Œ Enviar os dados para o banco
            print(f"\nğŸ“¤ Enviando {len(df)} registros para a tabela 'srag_warehouse'...")

            df.to_sql("srag_warehouse", con=engine, if_exists=if_exists, index=False, method="multi")

            # ğŸ“Œ Confirmar se os dados foram enviados corretamente
            print(f"\nâœ… Upload concluÃ­do com sucesso na tabela 'srag_warehouse'!")
            print(f"ğŸ”¹ Total de registros enviados: {len(df)}")

        except Exception as e:
            print(f"\nâŒ Erro ao enviar dados para o banco GCP: {str(e)}")

        print("\nğŸš€ Processo de envio finalizado.\n")
    
    def executar_classe(self):
        self.data_lake()
        self.ler_dataset()
        self.outliers()
        self.normalizacao()
        self.enviar_para_gcp(self.df)
    


    
    

# Criando uma instÃ¢ncia da classe Oficial e executando o pipeline corretamente
bot = Oficial()
bot.executar_classe()




