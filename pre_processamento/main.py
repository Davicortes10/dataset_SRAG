from pre_processamento import PreprocessDataset
from data_lake import Data_Lake
from dados_faltantes import Dados_Faltantes
from gcp_dataset import GCP_Dataset
from pre_processamento import PreprocessDataset
from outliers import Outliers
from normalizacao import Normalizacao
from sqlalchemy import create_engine

class Oficial:
    """
    Classe principal para gerenciar o processo de ETL (Extra√ß√£o, Tratamento e Carregamento de dados).
    
    Etapas:
    1Ô∏è‚É£ L√™ os dados do GCP atrav√©s da classe `GCP_Dataset`.
    2Ô∏è‚É£ Trata os dados faltantes usando a classe `Dados_Faltantes`.
    """

    def __init__(self):
        self.df = None
        self.bot_passo_1 = Data_Lake("/home/davicortes_oliveira1/dataset_SRAG/pre_processamento/srag.csv","srag_datalake")  # Inicializa sem valor 
    
    def data_lake(self):
        self.bot_passo_1.executar_datalake()

    def ler_dataset(self):
        """
        L√™ os dados do banco GCP e armazena no atributo `self.df`.
        Ap√≥s a leitura, inicializa `bot_passo_2` com `Dados_Faltantes(self.df)`.
        """
        print("üì• Lendo o dataset do GCP...")
        gcp = GCP_Dataset()
        self.df = gcp.ler_gcp_DB()

        if self.df is None or self.df.empty:
            print("‚ùå Erro: O dataset n√£o foi carregado corretamente!")
        else:
            print("‚úÖ Dataset carregado com sucesso!")
            bot = Normalizacao(self.df)
            dados_faltantes = Dados_Faltantes(self.df)  # Agora pode ser inicializado corretamente
            self.df = self.tratar_dados_faltantes(dados_faltantes)
            self.df = bot.normalizar_sexo_sinto(self.df)
            self.df = self.pre_processamento()

    def tratar_dados_faltantes(self, dados):
        """
        Executa o tratamento de dados faltantes.
        Verifica se o dataset foi carregado antes de iniciar o processamento.
        """
        if self.df is None or self.df.empty:
            print("‚ö†Ô∏è O dataset ainda n√£o foi carregado. Execute `ler_dataset()` primeiro.")
            return
        
        print("üîÑ Iniciando o tratamento de dados faltantes...")
        self.df = dados.executar_tratamento_de_dados_faltantes()
        print("‚úÖ Tratamento de dados conclu√≠do!")

        return self.df
    
    def pre_processamento(self):
        pre = PreprocessDataset(self.df)
        self.df = pre.converter_tipos_colunas()
        return self.df
    
    def outliers(self):
        oute = Outliers(self.df)
        self.df = oute.executar_outliers()
        self.df = self.pre_processamento()
    
    def normalizacao(self):
        bot = Normalizacao(self.df)
        self.df = bot.executar_normalizacao()
        self.df = self.pre_processamento()

    def enviar_para_gcp(self, df, if_exists="replace", batch_size=10000):
        """
        Envia um DataFrame Pandas para uma tabela MySQL no Google Cloud (GCP) usando chunks para evitar consumo excessivo de mem√≥ria.

        Par√¢metros:
        - df (pd.DataFrame): DataFrame a ser enviado para o banco de dados.
        - if_exists (str): Op√ß√£o de escrita na tabela ('fail', 'replace', 'append'). Padr√£o: 'replace'.
        - batch_size (int): Tamanho do chunk para inser√ß√£o dos dados (padr√£o: 10.000 linhas por batch).

        Retorna:
        - None
        """

        try:
            # Criar a conex√£o com o banco de dados
            print("üîó Conectando ao banco de dados GCP MySQL...")
            engine = create_engine("mysql+pymysql://devdavi:12345678@34.170.252.6/srag_warehouse")

            total_linhas = len(df)
            print(f"üì§ Enviando {total_linhas} registros para a tabela 'srag_warehouse' em chunks de {batch_size} linhas...")

            # Dividir a inser√ß√£o em chunks para evitar sobrecarga de mem√≥ria
            for i, chunk in enumerate(range(0, total_linhas, batch_size)):
                df_chunk = df.iloc[chunk : chunk + batch_size]  # Pegando um peda√ßo do DataFrame

                # Enviando para o banco
                df_chunk.to_sql("srag_warehouse", con=engine, if_exists=if_exists, index=False, method="multi")

                print(f"‚úÖ Chunk {i+1}: Inseriu {len(df_chunk)} registros (de {chunk} at√© {chunk+len(df_chunk)-1}).")

            print(f"\nüéâ Upload conclu√≠do com sucesso! Total de {total_linhas} registros inseridos.")

        except Exception as e:
            print(f"‚ùå Erro ao enviar dados para o banco GCP: {str(e)}")

    def executar_classe(self):
        #self.data_lake()
        self.ler_dataset()
        self.outliers()
        self.normalizacao()
        self.enviar_para_gcp(self.df)
    


    
    

# Criando uma inst√¢ncia da classe Oficial e executando o pipeline corretamente
bot = Oficial()
bot.executar_classe()




