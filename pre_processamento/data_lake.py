from sqlalchemy import create_engine
import pandas as pd
import pymysql
import time
from pyspark.sql import SparkSession
from pyspark.sql.types import StringType, StructField, StructType
from playwright.sync_api import sync_playwright


class Data_Lake:
    def __init__(self,caminho, db):
        self.file_path = "srag.csv"
        self.caminho = caminho
        self.db = db
        self.db_connection = f"jdbc:mysql://34.170.252.6:3306/{self.db}"
        self.spark = SparkSession.builder \
            .appName("Atualizar Data Lake") \
            .config("spark.jars", "/usr/share/java/mysql-connector-java-8.0.33.jar") \
        .getOrCreate()

    def automate_download(self):
        """
        Automatiza o download do arquivo SRAG do OpenDataSUS utilizando Playwright.

        Etapas do processo:
        1. Abre um navegador headless e acessa a URL do dataset SRAG-2020.
        2. Localiza e clica no primeiro link da pÃ¡gina.
        3. Aguarda a nova pÃ¡gina carregar completamente.
        4. Localiza e clica no link de download do arquivo.
        5. Salva o arquivo baixado no diretÃ³rio local.

        Tratamento de Erros:
        - Se houver erro ao acessar a pÃ¡gina ou localizar elementos, o navegador Ã© fechado corretamente.
        - Se o download falhar, exibe uma mensagem de erro.

        Retorna:
        - None. Apenas executa o download e salva o arquivo.

        DependÃªncias:
        - Instale o Playwright antes de executar: `pip install playwright`
        - Rode `playwright install` para instalar os navegadores.
        """

        print("ğŸ”„ Iniciando a automaÃ§Ã£o do download...")

        with sync_playwright() as p:
            try:
                print("ğŸŒ Inicializando navegador headless...")
                browser = p.chromium.launch(headless=True)  # Headless=True para rodar em segundo plano
                context = browser.new_context()
                page = context.new_page()

                # ğŸ“Œ Acessar a pÃ¡gina inicial
                url = "https://opendatasus.saude.gov.br/dataset/srag-2020"
                print(f"ğŸ” Acessando URL: {url}")
                page.goto(url, timeout=10000)  # Tempo limite de 10s para carregar a pÃ¡gina
                page.wait_for_load_state("load")  # Aguarde a pÃ¡gina carregar completamente
                print("âœ… PÃ¡gina carregada com sucesso!")

                # ğŸ“Œ Localizar e clicar no primeiro link (acessar pÃ¡gina de downloads)
                first_link_xpath = '//*[@id="dataset-resources"]/ul/li[3]/a'
                print(f"ğŸ” Aguardando elemento: {first_link_xpath}")
                page.wait_for_selector(first_link_xpath, timeout=5000)
                print("âœ… Elemento encontrado. Clicando...")
                page.locator(first_link_xpath).click()

                # ğŸ“Œ Aguarde a nova pÃ¡gina carregar
                page.wait_for_load_state("load")
                print("âœ… Nova pÃ¡gina carregada!")

                # ğŸ“Œ Localizar e clicar no link de download
                download_link_xpath = '//*[@id="content"]/div[3]/section/div/div[1]/ul/li/div/a'
                print(f"ğŸ” Aguardando elemento de download: {download_link_xpath}")
                page.wait_for_selector(download_link_xpath, timeout=5000)
                print("âœ… Elemento de download encontrado. Iniciando o download...")

                # ğŸ“Œ Iniciar o download e aguardar sua conclusÃ£o
                with page.expect_download() as download_info:
                    page.locator(download_link_xpath).click()
                download = download_info.value

                # ğŸ“Œ Salvar o arquivo baixado
                file_path = "srag.csv"
                download.save_as(file_path)
                print(f"ğŸ“ Download concluÃ­do! Arquivo salvo em: {file_path}")

            except Exception as e:
                print(f"âŒ Erro durante a automaÃ§Ã£o: {e}")

            finally:
                # ğŸš€ Garante que o navegador sempre seja fechado
                print("ğŸ›‘ Fechando o navegador...")
                browser.close()

    def atualizar_Data_Lake(self):
        """
        Atualiza o Data Lake carregando um arquivo CSV no PySpark e enviando os dados para o banco.

        Etapas do Processo:
        1ï¸âƒ£ Verifica se o arquivo existe antes de tentar carregar.
        2ï¸âƒ£ LÃª o arquivo CSV usando PySpark.
        3ï¸âƒ£ Exibe estatÃ­sticas do DataFrame carregado.
        4ï¸âƒ£ Conecta ao banco de dados e insere os dados.

        Tratamento de Erros:
        - Se o arquivo CSV nÃ£o for encontrado, exibe um erro e interrompe o processo.
        - Se houver falha na leitura do CSV, exibe um erro detalhado.
        - Se o PySpark nÃ£o estiver configurado corretamente, exibe uma mensagem de erro.

        ParÃ¢metros:
        - self.file_path (str): Caminho do arquivo CSV a ser carregado.
        - self.spark (SparkSession): SessÃ£o ativa do Spark.

        Retorna:
        - None. Apenas carrega os dados e atualiza o Data Lake.
        """

        print("ğŸ”„ Iniciando o processo de atualizaÃ§Ã£o do Data Lake com PySpark...")

        # ğŸš€ Verificar se o arquivo CSV existe antes de tentar carregar
        if not os.path.exists(self.caminho):
            print(f"âŒ Erro: O arquivo '{self.caminho}' nÃ£o foi encontrado.")
            return

        try:
            # ğŸ“Œ Ler o arquivo CSV usando PySpark
            print(f"ğŸ“‚ Lendo o arquivo CSV: {self.caminho}")
            df = self.spark.read.option("delimiter", ";") \
                .option("header", "true") \
                .csv(self.caminho)

            # ğŸš€ Verificar se o DataFrame foi carregado corretamente
            num_linhas = df.count()
            num_colunas = len(df.columns)

            if num_linhas == 0 or num_colunas == 0:
                print("âš ï¸ Aviso: O arquivo CSV foi carregado, mas estÃ¡ vazio!")
                return

            # ğŸ“Š Exibir informaÃ§Ãµes sobre os dados carregados
            print(f"âœ… Arquivo CSV carregado com sucesso!")
            print(f"ğŸ”¢ Total de Linhas: {num_linhas}")
            print(f"ğŸ“ Total de Colunas: {num_colunas}")
            print(f"ğŸ“‹ Colunas detectadas: {df.columns}")

            # ğŸ“Œ Chamar a funÃ§Ã£o para conectar ao banco de dados e inserir os dados
            self.conectar_DB(df)

        except Exception as e:
            print(f"âŒ Erro ao ler o arquivo CSV: {e}")
            return
    
    def conectar_DB(self, df):
        """
        Conecta ao banco de dados e realiza o upload dos dados do DataFrame para a tabela 'srag_datalake'.

        Etapas do Processo:
        1ï¸âƒ£ Verifica se o DataFrame contÃ©m dados antes de tentar salvar.
        2ï¸âƒ£ Configura a conexÃ£o JDBC para o MySQL.
        3ï¸âƒ£ Insere os dados no banco de dados no modo 'overwrite' (substituir existentes).
        4ï¸âƒ£ Exibe logs detalhados sobre o processo.

        SeguranÃ§a:
        - UsuÃ¡rio e senha sÃ£o recuperados de variÃ¡veis de ambiente para maior seguranÃ§a.

        Tratamento de Erros:
        - Se o DataFrame estiver vazio, exibe um alerta e interrompe o processo.
        - Se houver erro na conexÃ£o ou inserÃ§Ã£o, exibe uma mensagem detalhada.

        ParÃ¢metros:
        - self.db_connection (str): URL de conexÃ£o JDBC do banco de dados.
        - self.db (str): Nome da tabela no banco.
        - df (pyspark.sql.DataFrame): DataFrame PySpark contendo os dados a serem inseridos.

        Retorna:
        - None. Apenas faz o upload dos dados.
        """

        print("ğŸ”„ Iniciando o upload para o banco de dados...")

        # ğŸš€ VerificaÃ§Ã£o: O DataFrame contÃ©m dados?
        if df.count() == 0:
            print("âš ï¸ O DataFrame estÃ¡ vazio! Nenhum dado foi enviado ao banco.")
            return

        try:
            # ğŸ“Œ Credenciais armazenadas de forma segura
            db_user = os.getenv("DB_USER", "devdavi")  # Usa variÃ¡vel de ambiente ou valor padrÃ£o
            db_password = os.getenv("DB_PASSWORD", "12345678")  # Usa variÃ¡vel de ambiente ou valor padrÃ£o

            print(f"ğŸ“¡ Conectando ao banco de dados...")
            print(f"ğŸ”— URL: {self.db_connection}")
            print(f"ğŸ“‚ Tabela de destino: {self.db}")

            # ğŸ“Œ Escrever os dados no banco de dados
            df.write \
                .format("jdbc") \
                .option("url", self.db_connection) \
                .option("dbtable", f"{self.db}") \
                .option("user", db_user) \
                .option("password", db_password) \
                .option("driver", "com.mysql.cj.jdbc.Driver") \
                .mode("overwrite") \
                .save()

            print("âœ… Upload concluÃ­do com sucesso!")

        except Exception as e:
            print(f"âŒ Erro ao fazer o upload para o banco de dados: {str(e)}")
            return

        print("âœ… Processo de atualizaÃ§Ã£o do Data Lake finalizado com sucesso!")
    
    def executar_datalake(self):
        """
        Executa todo o pipeline do Data Lake, incluindo:
        
        1ï¸âƒ£ **Download AutomÃ¡tico dos Dados**
        2ï¸âƒ£ **AtualizaÃ§Ã£o do Data Lake**
        
        Etapas do Processo:
        - Chama `automate_download()` para baixar os dados do OpenDataSUS.
        - Chama `atualizar_Data_Lake()` para carregar os dados no sistema.
        
        Tratamento de Erros:
        - Se o download falhar, a atualizaÃ§Ã£o do Data Lake nÃ£o serÃ¡ executada.
        - Logs detalhados sÃ£o exibidos para facilitar a depuraÃ§Ã£o.
        
        Retorna:
        - None. Apenas executa as funÃ§Ãµes na sequÃªncia correta.
        """

        print("ğŸ”„ Iniciando o processo de execuÃ§Ã£o do Data Lake...")

        try:
            print("ğŸ“¥ Passo 1: Baixando os dados...")
            self.automate_download()
            print("âœ… Download concluÃ­do com sucesso!\n")

        except Exception as e:
            print(f"âŒ Erro ao baixar os dados: {str(e)}")
            print("ğŸš« Processo interrompido!")
            return  # Interrompe o fluxo se o download falhar

        try:
            print("ğŸ“Š Passo 2: Atualizando o Data Lake...")
            self.atualizar_Data_Lake()
            print("âœ… AtualizaÃ§Ã£o do Data Lake concluÃ­da com sucesso!")

        except Exception as e:
            print(f"âŒ Erro ao atualizar o Data Lake: {str(e)}")
            print("ğŸš« Processo interrompido!")
            return

        print("ğŸ âœ… Processo completo! O Data Lake foi atualizado com sucesso! ğŸš€")

